# 拓展脚本爬虫,爬取全部课程链接(一)

## 一、拓展脚本爬虫说明

在接下来的3节实验中，我们将对上一节写的脚本爬虫进行优化和扩展。

上一节的爬虫仅仅是爬取了课程页第一页全部课程的课程名、学习人数和课程标签。

接下来我们将从脚本爬虫的基础做起点，对实验楼所有的课程进行一个扫描，爬取课程的多个数据项：

- 实战一：课程名、课程图片链接、课程标签、学习人数
- 实战二：课程简介、实验列表、所属老师、课程所属分类，并且将这些数据存入到数据库sqlite3中
- 实战三：sql的介绍和使用

带着大家将Python基础、sql基础、BeautifulSoup基础都学习一遍。

---

## 二、项目实战

### 2.1 整体思路

1. 获取课程页面的全部链接
2. 获取当前页面的全部课程链接
3. 获取课程详细信息页面的有价值信息
4. 设计数据库表格，入库保存，为了分析数据而学习sql语法

---

### 2.2 项目实现

上一节的爬虫方式在本节不太适用，因为爬取任务不是一两个页面，所以这里采用函数定义的方式就够用了。

**思路一：** 下面开始写爬虫第一步，获取课程页面的全部页面链接（目前是24页，但是需要用爬虫来分析，因为课程持续添加中）
```
#文件名:shiyanlou_all_course.py
# 使用python3 shiyanlou_all_course.py运行
import requests
import re
from bs4 import BeautifulSoup
def main():
    res = requests.get('https://www.shiyanlou.com/courses/')#发起get将结果保存到res中
    soup = BeautifulSoup(res.text, 'lxml')
    course_link = "https://www.shiyanlou.com/courses/?course_type=all&tag=all&fee=all&page={}"#课程的链接都是这样的格式，首先定义好
    page = soup.find_all('ul',{'class':'pagination'})#获取底部导航栏的全部信息
    if len(page)<1:#判断，如果没拿到，爬虫无法继续，中断操作
        print('未获得全部页面')
        return None
    li_num = page[0].find_all('li')#查询底部li，因为数据保存在li中
    page_num = 0#首先设定page_num=0，页面数据肯定是大于0的
    for i in li_num:
        try:
            li_num = int(i.find('a').get_text())#获取放在a标签的字符串数字，并用int强制转换
        except:
            li_num = 0#如果获取失败，则赋值0
        if li_num > page_num:#page_num永远保存最大的值
            page_num = li_num
    # print(page_num,type(page_num))
    for i in range(1,page_num+1):#拿到page_num数字，从1开始数到page_num
        print(course_link.format(i))#打印课程页的全部链接
#下面代码的解释是，如果直接运行这个文件，下面的if判断成立。如果函数是被调用，则不运行main函数
if __name__ == "__main__":
    main()
```

思路解析：分析第一页、第二页、第三页的链接，全部是 `https://www.shiyanlou.com/courses/?course_type=all&tag=all&fee=all&page=数字` 形式，仔细一看其实就是get请求后面携带的参数，非常的简洁明了，所以我们需要从第一页中获取最大页码就可以了。

![scrapy-4-1](http://ov2hhj4hl.bkt.clouddn.com/scrapy-4-1.png)

查看html标签页写匹配规则

代码解析：

- 从课程首页开始，get请求获取网页的html代码，根据思路第一条获取最大页码数，从html源码查看，页面是在class等于pagination的ul标签中，如图

    ![scrapy-4-2](http://ov2hhj4hl.bkt.clouddn.com/scrapy-4-2.png)

    得到如下规则 `soup.find_all('ul',{'class':'pagination'})` ，这里返回的就是页面的全部html了，赋值给page变量。
- ul里面全部是li，li下有a标签，a里面的文本不一定全是数字，int()表示将括号内的变量转化为整型，如果不能转换就会报错，所以这里为了拿到最大的数字，还需要做异常处理，如果报错就赋值li_num为0，否则就是正常值。比较page_num和li_num，大的赋给page_num，这样一来就能拿到页面最大值。
- 在函数的开始，我们将页面全部链接通用部分已经定义好了，现在直接将数字用format()函数添加上去，就能得到全部。从1开始拿到页面最大值，range(起始值，结束值)是返回一个从起始到结束的数字列表，但包括起始不包括结束，所以从1到最大值的range写法是range(1,page_num+1)就可以了。
- 结果如下图
    ![scrapy-4-3](http://ov2hhj4hl.bkt.clouddn.com/scrapy-4-3.png)
    这里就打印了全部的链接，随便复制一个放浏览器的地址栏查看是否正确。

**思路二：** 爬取当前页面的全部课程链接，这也是简单的一个操作，拿到需要打开的链接，爬取全部课程链接就算完成了，但是值得注意的一点就是：课程详情页并没有学习人数，所以我们最好在这里先拿到学习人数并传给下一步。

另外，这个函数可以直接由上一个脚本爬虫外加一条规则爬取链接，然后直接做成接收url链接的函数就可以了，代码如下：
```
#这段代码在上一节已经解释，这里不再做解释
def get_course_link(url):
    res = requests.get(url)
    soup = BeautifulSoup(res.text, 'lxml')
    course = soup.find_all('div', {'class': 'col-md-3', 'class': 'col-sm-6', 'class': 'course'})
    for i in course:
        href = i.find('a',{'class':'course-box'}).get('href')
        title = i.find('span', {'class': 'course-title'}).get_text()
        study_people = i.find('span', {'class': 'course-per-num', 'class': 'pull-left'}).get_text()
        study_people = re.sub("\D", "", study_people)  # 数字这里有太多的空格和回车，清理一下
        try:
            tag = i.find('span', {'class': 'course-per-num', 'class': 'pull-right'}).get_text()
        except:
            tag = "课程"
        print("{}    学习人数:{}    {}   课程链接:{}\n".format(tag, study_people, title,host_url.format(href) ))
```